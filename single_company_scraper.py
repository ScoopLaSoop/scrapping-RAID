#!/usr/bin/env python3
"""
Scrapper pour une seule entreprise - Version ligne de commande
Usage: python single_company_scraper.py "Nom de l'entreprise"
"""

import asyncio
import logging
import sys
import json
from config import Config
from modules.company_scraper import CompanyScraper
from modules.api_legal_scraper import APILegalScraper

def setup_logging():
    """Configure le systÃ¨me de logging"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('single_company_scraping.log', encoding='utf-8')
        ]
    )

async def scrape_single_company(company_name: str, save_to_file: bool = True):
    """Scrappe une seule entreprise et retourne les rÃ©sultats"""
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸš€ DÃ©marrage du scrapping pour: {company_name}")
    
    try:
        # Configuration
        print("ğŸ”§ Chargement de la configuration...")
        config = Config()
        print("âœ… Configuration chargÃ©e")
        
        # Initialisation des scrappeurs
        print("âš™ï¸ Initialisation des scrappeurs...")
        company_scraper = CompanyScraper()
        api_legal_scraper = APILegalScraper()
        print("âœ… Scrappeurs initialisÃ©s")
        
        print(f"\nğŸ“‹ === Traitement de: {company_name} ===")
        
        # Ã‰TAPE 1: Scrapping web pour trouver la raison sociale
        print("ğŸŒ Ã‰TAPE 1: Scrapping web...")
        website_data = await company_scraper.scrape_company_website(company_name)
        
        # DÃ©terminer le nom Ã  utiliser pour l'API lÃ©gale
        if website_data and not website_data.get('error'):
            print(f"âœ… DonnÃ©es web rÃ©cupÃ©rÃ©es: {website_data}")
            
            # Extraire la raison sociale officielle
            official_name = website_data.get('raison_sociale', company_name)
            if official_name != company_name:
                print(f"ğŸ¯ Raison sociale officielle trouvÃ©e: {official_name}")
            else:
                print(f"âš ï¸ Raison sociale identique au nom commercial")
        else:
            print(f"âš ï¸ Scrapping web Ã©chouÃ©: {website_data.get('error', 'Erreur inconnue') if website_data else 'Erreur inconnue'}")
            print("ğŸ”„ FALLBACK: Utilisation du nom commercial pour l'API lÃ©gale")
            website_data = {}  # Pas de donnÃ©es web
            official_name = company_name  # Utiliser le nom commercial
            
        # Ã‰TAPE 2: Scrapping API lÃ©gale avec le nom dÃ©terminÃ©
        print(f"ğŸ›ï¸ Ã‰TAPE 2: Scrapping API lÃ©gale avec: {official_name}")
        legal_data = await api_legal_scraper.scrape_legal_info(official_name)
        
        # PrÃ©parer les rÃ©sultats finaux
        results = {
            'company_name': company_name,
            'official_name': official_name,
            'website_data': website_data,
            'legal_data': legal_data,
            'success': True
        }
        
        if legal_data and not legal_data.get('error'):
            print(f"âœ… DonnÃ©es lÃ©gales rÃ©cupÃ©rÃ©es: {legal_data}")
            logger.info(f"âœ… Entreprise {company_name} traitÃ©e avec succÃ¨s")
        else:
            print(f"âŒ Erreur API lÃ©gale: {legal_data}")
            logger.error(f"âŒ Erreur API lÃ©gale pour {company_name}: {legal_data}")
            results['success'] = False
        
        # Fermer les sessions
        print("\nğŸ”§ Fermeture des sessions...")
        await company_scraper.close_session()
        await api_legal_scraper.close_session()
        print("âœ… Sessions fermÃ©es")
        
        # Sauvegarder dans un fichier JSON si demandÃ©
        if save_to_file:
            filename = f"results_{company_name.replace(' ', '_').replace('/', '_')}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ RÃ©sultats sauvegardÃ©s dans: {filename}")
        
        # Afficher un rÃ©sumÃ©
        print("\n" + "="*50)
        print("ğŸ“Š RÃ‰SUMÃ‰ DES DONNÃ‰ES RÃ‰CUPÃ‰RÃ‰ES")
        print("="*50)
        
        if website_data and not website_data.get('error'):
            print("ğŸŒ DONNÃ‰ES WEB:")
            for key, value in website_data.items():
                if value:
                    print(f"  â€¢ {key}: {value}")
        
        if legal_data and not legal_data.get('error'):
            print("\nğŸ›ï¸ DONNÃ‰ES LÃ‰GALES:")
            for key, value in legal_data.items():
                if value:
                    print(f"  â€¢ {key}: {value}")
        
        print("\nğŸ‰ Scrapping terminÃ© avec succÃ¨s!")
        return results
        
    except Exception as e:
        logger.error(f"âŒ Erreur critique: {str(e)}")
        print(f"âŒ Erreur critique: {str(e)}")
        import traceback
        traceback.print_exc()
        return {'error': str(e), 'success': False}

def main():
    """Fonction principale avec gestion des arguments"""
    if len(sys.argv) < 2:
        print("âŒ Usage: python single_company_scraper.py \"Nom de l'entreprise\"")
        print("\nğŸ“‹ Exemples:")
        print("  python single_company_scraper.py \"ACOGEMAS\"")
        print("  python single_company_scraper.py \"Google France\"")
        print("  python single_company_scraper.py \"Maison d'Accueil SpÃ©cialisÃ©e\"")
        sys.exit(1)
    
    company_name = sys.argv[1]
    
    print("=== SCRAPPEUR ENTREPRISE UNIQUE ===")
    print(f"ğŸ¯ Entreprise cible: {company_name}")
    
    setup_logging()
    
    # ExÃ©cuter le scrapping
    results = asyncio.run(scrape_single_company(company_name))
    
    if results.get('success'):
        print(f"âœ… Scrapping rÃ©ussi pour: {company_name}")
    else:
        print(f"âŒ Ã‰chec du scrapping pour: {company_name}")
        sys.exit(1)

if __name__ == "__main__":
    main() 