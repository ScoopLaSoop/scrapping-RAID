#!/usr/bin/env python3
"""
Serveur webhook l√©ger pour Make.com (sans Chrome)
D√©clenche le scrapping en temps r√©el avec APIs uniquement
"""

import asyncio
import logging
import json
import os
from datetime import datetime
from aiohttp import web
from config import Config
from modules.airtable_client import AirtableClient
from modules.api_legal_scraper import APILegalScraper
from modules.solvability_checker import SolvabilityChecker

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('webhook_light.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class WebhookServerLight:
    def __init__(self):
        self.config = Config()
        # Mise √† jour temporaire de la cl√© API pour le test
        self.config.AIRTABLE_API_KEY = "patafQahY2YJYz2ZJ.0106dddf44d1c536bcefbd6566ecdd9861cb1f4461ecb31cf27fabb2a6132604"
        self.airtable_client = AirtableClient()
        self.api_legal_scraper = APILegalScraper()
        self.solvability_checker = SolvabilityChecker()
        
    async def handle_webhook(self, request):
        """G√®re les requ√™tes webhook de Make.com"""
        try:
            # R√©cup√©rer les donn√©es JSON
            data = await request.json()
            logger.info(f"üì• Webhook re√ßu: {data}")
            
            # Extraire le record ID
            record_id = data.get('record_id')
            if not record_id:
                return web.json_response({
                    'error': 'record_id manquant',
                    'status': 'error'
                }, status=400)
            
            # R√©cup√©rer l'entreprise depuis Airtable
            company = await self.airtable_client.get_company_by_id(record_id)
            if not company:
                return web.json_response({
                    'error': f'Entreprise non trouv√©e: {record_id}',
                    'status': 'error'
                }, status=404)
            
            company_name = company.get('name', '')
            logger.info(f"üìã Nom de l'entreprise r√©cup√©r√©: '{company_name}'")
            logger.info(f"üìã Tous les champs: {company.get('fields', {})}")
            logger.info(f"üè¢ Scrapping API de l'entreprise: {company_name}")
            
            # Lancer le scrapping API uniquement
            scraped_data = await self.scrape_single_company_api(company_name)
            
            # Mettre √† jour Airtable
            update_success = await self.airtable_client.update_company_data(record_id, scraped_data)
            
            logger.info(f"‚úÖ Scrapping API termin√© pour: {company_name}")
            
            # Envoyer les notifications vers Make.com si la mise √† jour Airtable a r√©ussi
            if update_success:
                await self.send_notifications_to_make(record_id, company_name)
            
            return web.json_response({
                'status': 'success',
                'company_name': company_name,
                'record_id': record_id,
                'scraped_data': scraped_data,
                'mode': 'api_only'
            })
            
        except Exception as e:
            logger.error(f"‚ùå Erreur webhook: {str(e)}")
            return web.json_response({
                'error': str(e),
                'status': 'error'
            }, status=500)
    
    async def scrape_single_company_api(self, company_name: str) -> dict:
        """Scrapping API uniquement d'une entreprise"""
        scraped_data = {}
        
        try:
            # 1. API l√©gale (SIRET, SIREN, TVA, etc.)
            logger.info(f"üèõÔ∏è API l√©gale pour: {company_name}")
            legal_data = await self.api_legal_scraper.scrape_legal_info(company_name)
            if legal_data:
                scraped_data['legal_data'] = legal_data
                logger.info(f"‚úÖ Donn√©es l√©gales r√©cup√©r√©es pour: {company_name}")
            
            # 2. V√©rification solvabilit√©
            logger.info(f"üè¶ V√©rification solvabilit√© pour: {company_name}")
            # Cr√©er un dictionnaire avec les donn√©es de l'entreprise
            company_data = {
                'name': company_name,
                'raison_sociale': company_name
            }
            solvability_data = await self.solvability_checker.check_company_solvability(company_data)
            if solvability_data:
                scraped_data['solvability_data'] = solvability_data
                logger.info(f"‚úÖ Donn√©es solvabilit√© r√©cup√©r√©es pour: {company_name}")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur scrapping API {company_name}: {str(e)}")
            scraped_data['error'] = str(e)
        
        return scraped_data
    
    async def send_notifications_to_make(self, record_id: str, company_name: str):
        """Envoie les notifications vers les webhooks Make.com"""
        try:
            # URLs des webhooks Make.com
            webhook_urls = [
                "https://hook.eu2.make.com/xntfo1d88k770bq8uykemtr7gvunh2y8",
                "https://hook.eu2.make.com/e86l71aeljrvlqvor4seahpv57gxl89b"
            ]
            
            # Donn√©es √† envoyer
            notification_data = {
                "record_id": record_id,
                "company_name": company_name,
                "scrapping_completed": True,
                "timestamp": datetime.now().isoformat(),
                "source": "scrapping-RAID-webhook"
            }
            
            logger.info(f"üì§ Envoi des notifications vers {len(webhook_urls)} webhooks Make.com")
            
            # Envoyer vers chaque webhook
            for i, url in enumerate(webhook_urls, 1):
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.post(url, json=notification_data, timeout=aiohttp.ClientTimeout(total=10)) as response:
                            if response.status == 200:
                                logger.info(f"‚úÖ Notification {i} envoy√©e avec succ√®s vers Make.com")
                            else:
                                logger.warning(f"‚ö†Ô∏è Erreur notification {i}: {response.status}")
                except Exception as e:
                    logger.error(f"‚ùå Erreur envoi notification {i}: {str(e)}")
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'envoi des notifications: {str(e)}")
    
    async def health_check(self, request):
        """Endpoint de sant√© pour Railway"""
        return web.json_response({
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'service': 'scrapping-RAID-webhook-light',
            'mode': 'api_only'
        })

async def main():
    """D√©marrage du serveur webhook l√©ger"""
    server = WebhookServerLight()
    
    # Cr√©er l'application web
    app = web.Application()
    
    # Routes
    app.router.add_post('/webhook', server.handle_webhook)
    app.router.add_get('/health', server.health_check)
    
    # Port (Railway utilise la variable PORT)
    port = int(os.environ.get('PORT', 8080))
    
    logger.info(f"üöÄ Serveur webhook l√©ger d√©marr√© sur le port {port}")
    logger.info(f"üì° Webhook URL: http://localhost:{port}/webhook")
    logger.info(f"üíö Health check: http://localhost:{port}/health")
    logger.info(f"üîß Mode: API uniquement (sans Chrome)")
    
    # D√©marrer le serveur
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', port)
    await site.start()
    
    # Garder le serveur actif
    try:
        await asyncio.Future()  # Attendre ind√©finiment
    except KeyboardInterrupt:
        logger.info("üõë Arr√™t du serveur webhook l√©ger")
    finally:
        await runner.cleanup()

if __name__ == '__main__':
    asyncio.run(main())
