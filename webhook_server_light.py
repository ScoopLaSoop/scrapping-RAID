#!/usr/bin/env python3
"""
Serveur webhook lÃ©ger pour Make.com (sans Chrome)
DÃ©clenche le scrapping en temps rÃ©el avec APIs uniquement
"""

import asyncio
import logging
import json
import os
from datetime import datetime
from aiohttp import web
from config import Config
from modules.airtable_client import AirtableClient
from modules.api_legal_scraper import APILegalScraper
from modules.solvability_checker import SolvabilityChecker

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('webhook_light.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class WebhookServerLight:
    def __init__(self):
        self.config = Config()
        self.airtable_client = AirtableClient()
        self.api_legal_scraper = APILegalScraper()
        self.solvability_checker = SolvabilityChecker()
        
    async def handle_webhook(self, request):
        """GÃ¨re les requÃªtes webhook de Make.com"""
        try:
            # RÃ©cupÃ©rer les donnÃ©es JSON
            data = await request.json()
            logger.info(f"ğŸ“¥ Webhook reÃ§u: {data}")
            
            # Extraire le record ID
            record_id = data.get('record_id')
            if not record_id:
                return web.json_response({
                    'error': 'record_id manquant',
                    'status': 'error'
                }, status=400)
            
            # RÃ©cupÃ©rer l'entreprise depuis Airtable
            company = await self.airtable_client.get_company_by_id(record_id)
            if not company:
                return web.json_response({
                    'error': f'Entreprise non trouvÃ©e: {record_id}',
                    'status': 'error'
                }, status=404)
            
            company_name = company.get('fields', {}).get('Nom de l\'entreprise', '')
            logger.info(f"ğŸ¢ Scrapping API de l'entreprise: {company_name}")
            
            # Lancer le scrapping API uniquement
            scraped_data = await self.scrape_single_company_api(company_name)
            
            # Mettre Ã  jour Airtable
            await self.airtable_client.update_company_data(record_id, scraped_data)
            
            logger.info(f"âœ… Scrapping API terminÃ© pour: {company_name}")
            
            return web.json_response({
                'status': 'success',
                'company_name': company_name,
                'record_id': record_id,
                'scraped_data': scraped_data,
                'mode': 'api_only'
            })
            
        except Exception as e:
            logger.error(f"âŒ Erreur webhook: {str(e)}")
            return web.json_response({
                'error': str(e),
                'status': 'error'
            }, status=500)
    
    async def scrape_single_company_api(self, company_name: str) -> dict:
        """Scrapping API uniquement d'une entreprise"""
        scraped_data = {}
        
        try:
            # 1. API lÃ©gale (SIRET, SIREN, TVA, etc.)
            logger.info(f"ğŸ›ï¸ API lÃ©gale pour: {company_name}")
            legal_data = await self.api_legal_scraper.get_company_info(company_name)
            if legal_data:
                scraped_data['legal_data'] = legal_data
                logger.info(f"âœ… DonnÃ©es lÃ©gales rÃ©cupÃ©rÃ©es pour: {company_name}")
            
            # 2. VÃ©rification solvabilitÃ©
            logger.info(f"ğŸ¦ VÃ©rification solvabilitÃ© pour: {company_name}")
            solvability_data = await self.solvability_checker.check_company_solvability(company_name)
            if solvability_data:
                scraped_data['solvability_data'] = solvability_data
                logger.info(f"âœ… DonnÃ©es solvabilitÃ© rÃ©cupÃ©rÃ©es pour: {company_name}")
                
        except Exception as e:
            logger.error(f"âŒ Erreur scrapping API {company_name}: {str(e)}")
            scraped_data['error'] = str(e)
        
        return scraped_data
    
    async def health_check(self, request):
        """Endpoint de santÃ© pour Railway"""
        return web.json_response({
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'service': 'scrapping-RAID-webhook-light',
            'mode': 'api_only'
        })

async def main():
    """DÃ©marrage du serveur webhook lÃ©ger"""
    server = WebhookServerLight()
    
    # CrÃ©er l'application web
    app = web.Application()
    
    # Routes
    app.router.add_post('/webhook', server.handle_webhook)
    app.router.add_get('/health', server.health_check)
    
    # Port (Railway utilise la variable PORT)
    port = int(os.environ.get('PORT', 8080))
    
    logger.info(f"ğŸš€ Serveur webhook lÃ©ger dÃ©marrÃ© sur le port {port}")
    logger.info(f"ğŸ“¡ Webhook URL: http://localhost:{port}/webhook")
    logger.info(f"ğŸ’š Health check: http://localhost:{port}/health")
    logger.info(f"ğŸ”§ Mode: API uniquement (sans Chrome)")
    
    # DÃ©marrer le serveur
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', port)
    await site.start()
    
    # Garder le serveur actif
    try:
        await asyncio.Future()  # Attendre indÃ©finiment
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ArrÃªt du serveur webhook lÃ©ger")
    finally:
        await runner.cleanup()

if __name__ == '__main__':
    asyncio.run(main())
