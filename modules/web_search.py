"""
Module de recherche web am√©lior√© pour trouver les sites d'entreprises
Utilise plusieurs m√©thodes et moteurs de recherche
"""

import aiohttp
import asyncio
import logging
import random
import time
from typing import Optional, List, Dict, Any
from config import Config
import difflib

logger = logging.getLogger(__name__)

class WebSearcher:
    def __init__(self):
        self.config = Config()
        self.session = None
        
        # Configuration Google Custom Search (√† ajouter dans config.py)
        self.google_api_key = getattr(self.config, 'GOOGLE_API_KEY', None)
        self.google_cx = getattr(self.config, 'GOOGLE_CX', None)
        
        # User agents rotatifs pour √©viter la d√©tection
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0'
        ]
        
    async def get_session(self):
        """Cr√©e ou retourne la session HTTP avec User-Agent rotatif"""
        if not self.session:
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30),
                headers=headers
            )
        return self.session
    
    async def close_session(self):
        """Ferme la session HTTP"""
        if self.session and not self.session.closed:
            await self.session.close()
            self.session = None
    
    def _generate_name_variants(self, company_name: str) -> List[str]:
        """G√©n√®re des variantes du nom d'entreprise pour la recherche"""
        variants = [company_name]
        
        # Variantes courantes
        name_lower = company_name.lower()
        
        # Supprimer les formes juridiques
        legal_forms = ['sarl', 'sas', 'sa', 'eurl', 'sasu', 'sci', 'association', 'ass']
        for form in legal_forms:
            if form in name_lower:
                variants.append(company_name.replace(form, '').replace(form.upper(), '').strip())
        
        # Acronymes -> mots complets (ex: ACOGEMAS -> ACO GEMAS)
        if len(company_name) > 3 and company_name.isupper():
            # Diviser l'acronyme en parties
            for i in range(2, len(company_name)-1):
                variant = company_name[:i] + ' ' + company_name[i:]
                variants.append(variant)
        
        # Mots similaires phon√©tiquement
        variants.extend(self._get_phonetic_variants(company_name))
        
        return list(set(variants))
    
    def _get_phonetic_variants(self, name: str) -> List[str]:
        """G√©n√®re des variantes phon√©tiques communes"""
        variants = []
        
        # Remplacements phon√©tiques courants
        phonetic_replacements = {
            'PH': 'F',
            'C': 'K',
            'QU': 'K',
            'X': 'KS',
            'Z': 'S',
            'J': 'G',
            'EMAS': 'EMAS',
            'EMAT': 'EMAS',
            'EGON': 'EGON',
            'EGOMA': 'EGOMA'
        }
        
        for old, new in phonetic_replacements.items():
            if old in name.upper():
                variants.append(name.upper().replace(old, new))
        
        return variants
    
    def _detect_organization_type(self, company_name: str) -> str:
        """D√©tecte le type d'organisation (entreprise, association, etc.)"""
        name_lower = company_name.lower()
        
        # Mots cl√©s d'associations
        association_keywords = [
            'association', 'ass', 'amicale', 'club', 'federation', 'syndicat',
            'comite', 'collectif', 'groupement', 'union', 'confederation',
            'maison', 'accueil', 'specialise', 'pupilles', 'enseignement'
        ]
        
        association_score = sum(1 for keyword in association_keywords if keyword in name_lower)
        
        if association_score >= 2:
            return 'association'
        elif any(word in name_lower for word in ['sarl', 'sas', 'sa', 'eurl', 'sasu']):
            return 'company'
        else:
            return 'unknown'
    
    async def search_company_website(self, company_name: str) -> Optional[str]:
        """
        Recherche optimis√©e du site web d'une entreprise
        Version acc√©l√©r√©e : URL directe + Bing uniquement
        """
        logger.info(f"üîç Recherche du site web pour: {company_name}")
        
        # √âTAPE 1: Test direct des URLs probables (le plus rapide)
        logger.info(f"üéØ Recherche du nom exact: {company_name}")
        result = await self._try_direct_url_variants(company_name)
        if result:
            return result
        
        # √âTAPE 2: Bing uniquement (plus rapide que DuckDuckGo)
        result = await self._search_via_bing(company_name)
        if result:
            return result
        
        # √âTAPE 3: Test des variantes du nom (URL directe + Bing)
        logger.info(f"‚ö†Ô∏è Nom exact non trouv√©, test des variantes...")
        name_variants = self._generate_name_variants(company_name)
        name_variants = [v for v in name_variants if v != company_name]
        
        if name_variants:
            logger.info(f"üîÑ Variantes √† tester: {name_variants[:2]}...")
            
            # Tester seulement les 2 premi√®res variantes
            for i, variant in enumerate(name_variants[:2]):
                logger.info(f"üîç Test variante {i+1}: {variant}")
                
                # Test direct URL uniquement pour les variantes
                result = await self._try_direct_url_variants(variant)
                if result:
                    return result
                
                # Test Bing pour les variantes
                result = await self._search_via_bing(variant)
                if result:
                    return result
                
                # Pause minimale entre les variantes
                await asyncio.sleep(0.5)
        
        logger.warning(f"‚ö†Ô∏è Aucun site trouv√© pour {company_name}")
        return None
    
    async def _try_direct_url_variants(self, company_name: str) -> Optional[str]:
        """Teste directement des variantes d'URL probables"""
        logger.info(f"üéØ Test des variantes d'URL directes pour: {company_name}")
        
        # Nettoyer le nom pour cr√©er des URLs
        clean_name = self._clean_name_for_url(company_name)
        
        # Variantes d'URL √† tester
        url_variants = [
            f"https://www.{clean_name}.fr",
            f"https://www.{clean_name}.com",
            f"https://{clean_name}.fr",
            f"https://{clean_name}.com",
            f"https://www.{clean_name}.net",
            f"https://www.{clean_name}.org"
        ]
        
        for url in url_variants:
            try:
                session = await self.get_session()
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=3)) as response:
                    if response.status == 200:
                        # V√©rification rapide de pertinence
                        if await self._verify_website_relevance_fast(url, company_name):
                            logger.info(f"‚úÖ URL directe trouv√©e: {url}")
                            return url
                        else:
                            logger.info(f"‚ùå URL trouv√©e mais non pertinente: {url}")
            except:
                continue
            
            # Pause minimale
            await asyncio.sleep(0.2)
        
        logger.info(f"‚ùå Aucune URL directe trouv√©e pour: {company_name}")
        return None
    
    def _clean_name_for_url(self, company_name: str) -> str:
        """Nettoie un nom d'entreprise pour cr√©er une URL"""
        # Supprimer les mots courants
        words_to_remove = [
            'sarl', 'sas', 'sa', 'eurl', 'sasu', 'sci',
            'soci√©t√©', 'entreprise', 'ets', 'etablissements',
            'groupe', 'compagnie', 'cie', 'et', 'de', 'la', 'le', 'les',
            'du', 'des', 'au', 'aux', 'pour', 'avec', 'sans'
        ]
        
        name = company_name.lower()
        
        # Supprimer les mots courants
        for word in words_to_remove:
            name = name.replace(f' {word} ', ' ')
            name = name.replace(f' {word}', '')
            name = name.replace(f'{word} ', '')
        
        # Nettoyer les caract√®res sp√©ciaux
        import re
        name = re.sub(r'[^a-zA-Z0-9\s-]', '', name)
        
        # Remplacer espaces par tirets
        name = re.sub(r'\s+', '-', name.strip())
        
        # Supprimer les tirets multiples
        name = re.sub(r'-+', '-', name)
        
        return name.strip('-')
    
    async def _search_via_bing(self, company_name: str) -> Optional[str]:
        """Recherche via Bing (optimis√©e pour la vitesse)"""
        try:
            logger.info(f"üîç Recherche Bing pour: {company_name}")
            session = await self.get_session()
            
            # Construire la requ√™te
            query = f'"{company_name}" site officiel'
            url = f"https://www.bing.com/search"
            
            params = {
                'q': query,
                'setlang': 'fr',
                'cc': 'FR'
            }
            
            # D√©lai minimal pour √©viter la d√©tection
            await asyncio.sleep(random.uniform(0.5, 1.5))
            
            async with session.get(url, params=params) as response:
                if response.status == 200:
                    html = await response.text()
                    
                    # Parser les r√©sultats Bing
                    urls = self._extract_urls_from_bing_html(html)
                    
                    for url in urls[:5]:  # Tester les 5 premiers
                        if self._is_relevant_website(url, "", company_name):
                            if await self._test_website_access(url):
                                logger.info(f"‚úÖ Site trouv√© via Bing: {url}")
                                return url
                    
                    logger.info(f"‚ùå Aucun site pertinent trouv√© via Bing pour {company_name}")
                    return None
                else:
                    logger.error(f"‚ùå Erreur Bing: {response.status}")
                    return None
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur recherche Bing pour {company_name}: {str(e)}")
            return None
    
    def _extract_urls_from_bing_html(self, html: str) -> List[str]:
        """Extrait les URLs des r√©sultats Bing"""
        from bs4 import BeautifulSoup
        import re
        
        soup = BeautifulSoup(html, 'html.parser')
        urls = []
        
        # Chercher les liens dans les r√©sultats Bing
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href and href.startswith('http') and 'bing.com' not in href and 'microsoft.com' not in href:
                urls.append(href)
        
        return urls
    
    async def _search_via_duckduckgo_improved(self, company_name: str) -> Optional[str]:
        """Recherche via DuckDuckGo avec gestion am√©lior√©e des erreurs et proxies"""
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                logger.info(f"üîç Recherche DuckDuckGo pour: {company_name} (tentative {attempt + 1}/{max_attempts})")
                
                # Rotation des User-Agents √† chaque tentative
                session = await self.get_session()
                session.headers['User-Agent'] = random.choice(self.user_agents)
                
                # Construire la requ√™te avec des termes fran√ßais
                query = f'"{company_name}" site:fr OR inurl:fr OR "France" OR "fran√ßais"'
                url = "https://duckduckgo.com/html/"
                
                params = {
                    'q': query,
                    'kl': 'fr-fr',  # R√©gion fran√ßaise
                    'ia': 'web',
                    'safe': 'moderate'
                }
                
                # D√©lai al√©atoire croissant √† chaque tentative
                delay = random.uniform(5, 15) * (attempt + 1)
                logger.info(f"‚è≥ Attente de {delay:.1f}s avant retry...")
                await asyncio.sleep(delay)
                
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        html = await response.text()
                        
                        # Parser les r√©sultats
                        urls = self._extract_urls_from_duckduckgo_html(html)
                        
                        for url in urls[:5]:  # Tester les 5 premiers
                            if self._is_relevant_website(url, "", company_name):
                                if await self._test_website_access(url):
                                    logger.info(f"‚úÖ Site trouv√© via DuckDuckGo: {url}")
                                    return url
                        
                        logger.info(f"‚ùå Aucun site pertinent trouv√© via DuckDuckGo pour {company_name}")
                        return None
                        
                    elif response.status == 202:
                        logger.warning(f"‚ö†Ô∏è Erreur DuckDuckGo: {response.status} (tentative {attempt + 1})")
                        if attempt < max_attempts - 1:
                            # Attendre plus longtemps avant le retry
                            await asyncio.sleep(random.uniform(10, 20))
                            continue
                        else:
                            logger.error(f"‚ùå DuckDuckGo bloqu√© apr√®s {max_attempts} tentatives")
                            return None
                    else:
                        logger.error(f"‚ùå Erreur DuckDuckGo: {response.status}")
                        return None
                        
            except Exception as e:
                logger.error(f"‚ùå Erreur recherche DuckDuckGo (tentative {attempt + 1}): {str(e)}")
                if attempt < max_attempts - 1:
                    await asyncio.sleep(random.uniform(5, 10))
                    continue
                else:
                    return None
        
        return None
    
    async def _search_professional_directories(self, company_name: str) -> Optional[str]:
        """Recherche dans les annuaires professionnels fran√ßais"""
        logger.info(f"üìã Recherche dans les annuaires pour: {company_name}")
        
        # Annuaires fran√ßais √† consulter
        directories = [
            "https://www.pagesjaunes.fr/pagesblanches/recherche",
            "https://www.societe.com/cgi-bin/search",
            "https://www.verif.com/Search"
        ]
        
        # Cette m√©thode pourrait √™tre d√©velopp√©e pour parser les annuaires
        # Pour l'instant, on retourne None (√† impl√©menter si besoin)
        return None
    
    async def _verify_website_relevance_fast(self, url: str, company_name: str) -> bool:
        """V√©rification rapide de pertinence (version optimis√©e)"""
        try:
            session = await self.get_session()
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=5)) as response:
                if response.status == 200:
                    html = await response.text()
                    html_lower = html.lower()
                    
                    # V√©rification rapide : nom de l'entreprise + indicateurs fran√ßais
                    company_words = [word.lower() for word in company_name.split() if len(word) > 2]
                    company_matches = sum(1 for word in company_words if word in html_lower)
                    
                    french_indicators = ['france', 'fran√ßais', 'fr', 'siret', 'siren', 'tva']
                    french_score = sum(1 for indicator in french_indicators if indicator in html_lower)
                    
                    # Validation rapide
                    is_relevant = (
                        company_matches >= len(company_words) * 0.3 and  # Au moins 30% des mots
                        french_score >= 1 and  # Au moins un indicateur fran√ßais
                        '.fr' in url  # Domaine fran√ßais
                    )
                    
                    return is_relevant
                else:
                    return False
        except Exception as e:
            logger.error(f"‚ùå Erreur validation rapide {url}: {str(e)}")
            return False
    
    async def _verify_website_relevance(self, url: str, company_name: str) -> bool:
        """V√©rifie si un site web contient bien des informations sur l'entreprise fran√ßaise"""
        try:
            session = await self.get_session()
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                if response.status == 200:
                    html = await response.text()
                    html_lower = html.lower()
                    
                    # 1. V√©rifier que c'est un site fran√ßais
                    french_indicators = [
                        'france', 'fran√ßais', 'francais', 'fr', 
                        'mentions l√©gales', 'mentions legales',
                        'siret', 'siren', 'tva', 'intracom',
                        'adresse', 't√©l√©phone', 'telephone'
                    ]
                    
                    french_score = sum(1 for indicator in french_indicators if indicator in html_lower)
                    
                    # 2. V√©rifier la pr√©sence du nom de l'entreprise
                    company_words = [word.lower() for word in company_name.split() if len(word) > 2]
                    company_matches = sum(1 for word in company_words if word in html_lower)
                    
                    # 3. V√©rifier des √©l√©ments commerciaux fran√ßais
                    business_indicators = [
                        'contact', 'services', 'produits', 'soci√©t√©', 'entreprise',
                        'accueil', '√† propos', 'qui sommes-nous', 'notre √©quipe'
                    ]
                    
                    business_score = sum(1 for indicator in business_indicators if indicator in html_lower)
                    
                    # 4. Exclure les sites √©trangers √©vidents
                    foreign_indicators = [
                        'united states', 'usa', 'america', 'uk', 'england',
                        'germany', 'deutschland', 'spain', 'espana', 'italy'
                    ]
                    
                    has_foreign = any(indicator in html_lower for indicator in foreign_indicators)
                    
                    # 5. V√©rifier l'extension du domaine
                    domain_score = 2 if '.fr' in url else (1 if '.com' in url else 0)
                    
                    # Calcul du score de pertinence
                    total_score = french_score + (company_matches * 2) + business_score + domain_score
                    
                    # Conditions pour valider le site
                    is_relevant = (
                        total_score >= 5 and  # Score minimum
                        company_matches >= len(company_words) * 0.5 and  # Au moins 50% des mots de l'entreprise
                        not has_foreign and  # Pas d'indicateurs √©trangers
                        french_score >= 1  # Au moins un indicateur fran√ßais
                    )
                    
                    logger.info(f"üîç Pertinence {url}: score={total_score}, entreprise={company_matches}/{len(company_words)}, fran√ßais={french_score}, √©tranger={has_foreign} ‚Üí {'‚úÖ' if is_relevant else '‚ùå'}")
                    
                    return is_relevant
                else:
                    return False
        except Exception as e:
            logger.error(f"‚ùå Erreur validation pertinence {url}: {str(e)}")
            return False
    
    async def _search_via_google(self, company_name: str) -> Optional[str]:
        """Recherche via Google Custom Search API"""
        try:
            session = await self.get_session()
            
            # Construire la requ√™te de recherche
            query = f'"{company_name}" site officiel'
            url = f"https://www.googleapis.com/customsearch/v1"
            
            params = {
                'key': self.google_api_key,
                'cx': self.google_cx,
                'q': query,
                'num': 5,
                'lr': 'lang_fr'  # Privil√©gier les sites fran√ßais
            }
            
            async with session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    # Analyser les r√©sultats
                    items = data.get('items', [])
                    for item in items:
                        url = item.get('link', '')
                        title = item.get('title', '')
                        
                        # V√©rifier si c'est un site pertinent
                        if self._is_relevant_website(url, title, company_name):
                            # V√©rifier que le site est accessible
                            if await self._test_website_access(url):
                                logger.info(f"üîç Site trouv√© via Google: {url}")
                                return url
                    
                    logger.warning(f"‚ö†Ô∏è Aucun site pertinent trouv√© via Google pour {company_name}")
                    return None
                else:
                    logger.error(f"‚ùå Erreur Google Search API: {response.status}")
                    return None
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur recherche Google pour {company_name}: {str(e)}")
            return None
    
    def _extract_urls_from_duckduckgo_html(self, html: str) -> List[str]:
        """Extrait les URLs des r√©sultats DuckDuckGo"""
        from bs4 import BeautifulSoup
        import re
        
        soup = BeautifulSoup(html, 'html.parser')
        urls = []
        
        # Chercher les liens dans les r√©sultats
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href and href.startswith('http') and 'duckduckgo.com' not in href:
                urls.append(href)
        
        return urls
    
    def _is_relevant_website(self, url: str, title: str, company_name: str) -> bool:
        """V√©rifie si un site web est pertinent pour l'entreprise (version rapide)"""
        # Filtrer les sites non pertinents
        blacklist = [
            'facebook.com', 'linkedin.com', 'twitter.com', 'instagram.com',
            'youtube.com', 'google.com', 'wikipedia.org', 'pages-jaunes.fr',
            'societe.com', 'verif.com', 'infogreffe.fr', 'company.com',
            'glassdoor.com', 'indeed.com', 'jobijoba.com', 'monster.fr'
        ]
        
        # Exclure les domaines √©trangers √©vidents
        foreign_domains = [
            '.us', '.uk', '.de', '.es', '.it', '.ca', '.au', '.be', '.nl'
        ]
        
        url_lower = url.lower()
        
        # V√©rifier blacklist
        for blocked in blacklist:
            if blocked in url_lower:
                return False
        
        # V√©rifier domaines √©trangers
        for foreign in foreign_domains:
            if foreign in url_lower:
                return False
        
        # Privil√©gier les domaines fran√ßais
        if '.fr' in url_lower:
            preference_bonus = 2
        elif '.com' in url_lower:
            preference_bonus = 1
        else:
            preference_bonus = 0
        
        # V√©rifier si le nom de l'entreprise est dans l'URL ou le titre
        company_clean = company_name.lower().replace(' ', '').replace('-', '').replace('.', '')
        url_clean = url_lower.replace(' ', '').replace('-', '').replace('.', '')
        title_clean = title.lower().replace(' ', '').replace('-', '').replace('.', '')
        
        # Recherche flexible
        if company_clean in url_clean or company_clean in title_clean:
            return True
        
        # Recherche par mots-cl√©s avec bonus pour domaines fran√ßais
        company_words = company_name.lower().split()
        url_words = url_lower.split('/')
        title_words = title.lower().split()
        
        matches = 0
        for word in company_words:
            if len(word) > 3:  # Ignorer les mots trop courts
                if any(word in url_word for url_word in url_words) or any(word in title_word for title_word in title_words):
                    matches += 1
        
        required_match_ratio = max(0.4, 0.8 - (preference_bonus * 0.2))  # Plus flexible pour .fr
        return matches >= len(company_words) * required_match_ratio
    
    async def _test_website_access(self, url: str) -> bool:
        """Teste si un site web est accessible"""
        try:
            session = await self.get_session()
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                return response.status == 200
        except:
            return False
    
    async def _search_via_alternative_engines(self, company_name: str) -> Optional[str]:
        """Recherche via des moteurs alternatifs quand les principaux √©chouent"""
        alternative_engines = [
            ('Startpage', 'https://startpage.com/sp/search'),
            ('Searx', 'https://searx.be/search'),
            ('Yandex', 'https://yandex.com/search/')
        ]
        
        for engine_name, base_url in alternative_engines:
            try:
                logger.info(f"üîç Recherche via {engine_name} pour: {company_name}")
                
                session = await self.get_session()
                query = f'"{company_name}" site officiel France'
                
                params = {
                    'q': query,
                    'lang': 'fr',
                    'category_general': '1'
                }
                
                async with session.get(base_url, params=params) as response:
                    if response.status == 200:
                        html = await response.text()
                        
                        # Parser les r√©sultats (m√©thode g√©n√©rique)
                        urls = self._extract_urls_from_generic_html(html)
                        
                        for url in urls[:3]:  # Tester les 3 premiers
                            if self._is_relevant_website(url, "", company_name):
                                if await self._test_website_access(url):
                                    logger.info(f"‚úÖ Site trouv√© via {engine_name}: {url}")
                                    return url
                
                # Pause entre les moteurs
                await asyncio.sleep(random.uniform(3, 7))
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur {engine_name}: {str(e)}")
                continue
        
        return None
    
    def _extract_urls_from_generic_html(self, html: str) -> List[str]:
        """Extrait les URLs de n'importe quel moteur de recherche"""
        import re
        
        # Patterns pour trouver les URLs
        url_patterns = [
            r'href="(https?://[^"]+)"',
            r'href=\'(https?://[^\']+)\'',
            r'url":"(https?://[^"]+)"',
            r'(https?://[^\s<>"]+\.[a-zA-Z]{2,})'
        ]
        
        urls = []
        for pattern in url_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            urls.extend(matches)
        
        # Nettoyer et filtrer les URLs
        clean_urls = []
        for url in urls:
            # Supprimer les URLs internes aux moteurs de recherche
            skip_domains = [
                'google.com', 'bing.com', 'duckduckgo.com', 'yahoo.com',
                'startpage.com', 'searx.be', 'yandex.com', 'wikipedia.org',
                'facebook.com', 'twitter.com', 'linkedin.com', 'youtube.com'
            ]
            
            if not any(domain in url.lower() for domain in skip_domains):
                if url.startswith(('http://', 'https://')):
                    clean_urls.append(url)
        
        return list(set(clean_urls))[:10]  # Retourner max 10 URLs uniques 