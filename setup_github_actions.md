# ğŸš€ Guide GitHub Actions - HÃ©bergement 100% GRATUIT

## ğŸ¯ Pourquoi GitHub Actions ?

- âœ… **Totalement gratuit** (2000 minutes/mois mÃªme pour les repos privÃ©s)
- âœ… **Cron jobs intÃ©grÃ©s** (planification automatique)
- âœ… **Aucun serveur Ã  maintenir**
- âœ… **Logs dÃ©taillÃ©s** et historique
- âœ… **Docker support** natif
- âœ… **Variables d'environnement** sÃ©curisÃ©es

## ğŸ“‹ Ã‰tapes de configuration

### 1. CrÃ©er un repository GitHub

```bash
# Initialiser Git (si pas dÃ©jÃ  fait)
git init

# Ajouter tous les fichiers
git add .

# Premier commit
git commit -m "Initial commit - Bot de scrapping"

# CrÃ©er le repo sur GitHub (remplacez par votre nom d'utilisateur)
# Puis ajouter l'origine
git remote add origin https://github.com/VOTRE-USERNAME/scrapping-RAID.git
git push -u origin main
```

### 2. Configurer les secrets GitHub

1. Aller sur votre repository GitHub
2. Cliquer sur **Settings** > **Secrets and variables** > **Actions**
3. Cliquer sur **New repository secret**
4. Ajouter chaque secret :

| Nom du secret | Valeur |
|--------------|--------|
| `AIRTABLE_API_KEY` | Votre clÃ© API Airtable |
| `AIRTABLE_BASE_ID` | ID de votre base Airtable |
| `AIRTABLE_TABLE_NAME` | Nom de votre table |
| `AIRTABLE_VIEW_NAME` | Nom de votre vue (ex: "Scrapping") |
| `OPENAI_API_KEY` | Votre clÃ© OpenAI |
| `OPENAI_ORG_ID` | Votre Organization ID OpenAI |
| `MAKE_WEBHOOK_URL` | URL de votre webhook Make (optionnel) |

### 3. Pousser la configuration GitHub Actions

```bash
# Ajouter les nouveaux fichiers Docker
git add Dockerfile .github/ docker-compose.yml .dockerignore

# Commit
git commit -m "Ajout configuration Docker et GitHub Actions"

# Push
git push origin main
```

### 4. Tester l'exÃ©cution

1. Aller sur votre repository GitHub
2. Cliquer sur **Actions**
3. SÃ©lectionner **Bot de Scrapping Quotidien**
4. Cliquer sur **Run workflow** > **Run workflow**
5. Attendre l'exÃ©cution (2-3 minutes)

## ğŸ”§ Configuration avancÃ©e

### Modifier l'heure d'exÃ©cution

Dans `.github/workflows/scraper.yml`, modifiez la ligne :
```yaml
- cron: '0 9 * * *'  # 9h UTC = 10h Paris (hiver) / 11h Paris (Ã©tÃ©)
```

Exemples d'heures :
- `0 8 * * *` : 8h UTC (9h/10h Paris)
- `0 10 * * *` : 10h UTC (11h/12h Paris)
- `0 6 * * 1-5` : 6h UTC du lundi au vendredi seulement

### Notifications d'erreur

Ajoutez cette Ã©tape dans le workflow pour recevoir des notifications :

```yaml
- name: ğŸ“§ Notification d'erreur
  if: failure()
  run: |
    echo "âŒ Erreur dÃ©tectÃ©e lors de l'exÃ©cution"
    echo "Consultez les logs dans l'onglet Actions"
```

### Limitation des ressources

Pour Ã©conomiser les minutes gratuites, ajoutez :

```yaml
- name: ğŸ”„ Limitation des entreprises
  run: |
    docker run --rm \
      -e MAX_COMPANIES=10 \
      # ... autres variables
```

## ğŸ” Tests locaux avec Docker

### Construction et test

```bash
# Construction de l'image
docker build -t scraper-bot .

# Test local
docker run --rm \
  --env-file .env \
  scraper-bot

# Ou avec Docker Compose
docker-compose up
```

### Debug

```bash
# ExÃ©cution interactive
docker run -it --rm \
  --env-file .env \
  scraper-bot bash

# VÃ©rification de Chrome
docker run --rm scraper-bot google-chrome --version
```

## ğŸ“Š Monitoring

### Consulter les logs

1. GitHub > Repository > Actions
2. Cliquer sur l'exÃ©cution
3. DÃ©velopper "ğŸš€ ExÃ©cution du bot de scrapping"
4. Voir les logs dÃ©taillÃ©s

### Historique des exÃ©cutions

GitHub garde l'historique complet :
- âœ… ExÃ©cutions rÃ©ussies
- âŒ Erreurs avec dÃ©tails
- â±ï¸ DurÃ©e d'exÃ©cution
- ğŸ“Š Nombre d'entreprises traitÃ©es

## ğŸ¯ Avantages vs InconvÃ©nients

### âœ… Avantages

- **Gratuit** : 2000 minutes/mois
- **Fiable** : Infrastructure GitHub
- **SÃ©curisÃ©** : Secrets chiffrÃ©s
- **Logs** : Historique complet
- **FlexibilitÃ©** : Modification facile

### âš ï¸ Limitations

- **DurÃ©e max** : 6 heures par job
- **Stockage** : Pas de persistance entre runs
- **RÃ©seau** : Pas d'IP fixe
- **Ressources** : 2 CPU cores, 7GB RAM

## ğŸš€ Optimisations

### RÃ©duire le temps d'exÃ©cution

1. **Limiter les entreprises** par run
2. **Cache Docker** pour les builds
3. **ParallÃ©lisation** des tÃ¢ches
4. **API uniquement** (pas de scraping web)

### Exemple d'optimisation

```yaml
- name: ğŸ”§ Construction optimisÃ©e
  run: |
    docker build --cache-from scraper-bot -t scraper-bot .
```

## ğŸ“± Alternative : Self-hosted runners

Si vous avez un serveur/PC qui tourne 24/7 :

```bash
# Installer un runner GitHub sur votre machine
# Settings > Actions > Runners > New self-hosted runner
```

**Avantages** :
- Temps d'exÃ©cution illimitÃ©
- Pas de limite de minutes
- Performances locales

## ğŸ‰ RÃ©sumÃ©

**Votre bot tournera automatiquement :**
- ğŸ•˜ **Tous les jours Ã  9h** (modifiable)
- ğŸ†“ **Totalement gratuit**
- ğŸ“Š **Logs dÃ©taillÃ©s** disponibles
- ğŸ”’ **SÃ©curisÃ©** avec secrets GitHub
- ğŸš€ **Mise Ã  jour facile** via Git

**Prochaines Ã©tapes :**
1. Pusher le code sur GitHub
2. Configurer les secrets
3. Tester une premiÃ¨re exÃ©cution
4. Laisser tourner automatiquement !

Le bot est maintenant autonome et ne nÃ©cessite plus votre ordinateur ! ğŸ¯ 